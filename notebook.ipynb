{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IT Ticket Classifier — DHAUZ Challenge\n",
        "\n",
        "Notebook executável: carrega o dataset, amostra 200 tickets, monta o RAG (embeddings + FAISS), executa o fluxo LangGraph em exemplos e na amostra, calcula métricas.\n",
        "Arquitetura baseada em pesquisa com experimentação em \"labs/architecture-comparison.ipynb\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resultados do labs (architecture-comparison)\n",
        "\n",
        "Gráfico de threshold KNN (cruzamento coverage × F1 macro) e tabela comparativa de modelos no conjunto de teste (hold-out excl. sample_200).\n",
        "\n",
        "![KNN: métricas vs confidence threshold](charts/threshold.png)\n",
        "Resultado mostra que threshold para maior coverage e maio f1 macro é 0.45\n",
        "Cruzamento (coverage ≈ F1 macro): threshold=0.45 → coverage=90.35%, F1 macro=0.8697, accuracy=0.8680\n",
        "Melhor equilíbrio (max coverage×F1): threshold=0.35 → coverage=95.31%, F1 macro=0.8450, accuracy=0.8424\n",
        "\n",
        "**Tabela comparativa (test set):**\n",
        "\n",
        "| model    | accuracy | f1_macro | f1_weighted |\n",
        "|----------|----------|----------|-------------|\n",
        "| RNN      | 0.772571 | 0.763023 | 0.773050    |\n",
        "| LSTM     | 0.800138 | 0.794236 | 0.801268    |\n",
        "| BiLSTM   | 0.831840 | 0.835082 | 0.832611    |\n",
        "| BiGRU    | 0.833218 | 0.838499 | 0.833300    |\n",
        "| CNN+BiGRU| 0.837354 | 0.842469 | 0.837848    |\n",
        "| KNN      | 0.833218 | 0.832331 | 0.832036    |\n",
        "\n",
        "### usaremos KNN pelo motivo de ser mais explicável e permitir construir uma base de conhecimento muito granular, veja mais em labs/architecture-comparison.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\".\").resolve()\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(ROOT / \".env\")\n",
        "\n",
        "import config\n",
        "np.random.seed(config.SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Baixar dataset do Kaggle (só se ainda não tiver o CSV em data/raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset em: /Users/moises/Documents/ticket-classifier/data/raw/all_tickets_processed_improved_v3.csv\n"
          ]
        }
      ],
      "source": [
        "from src.prep import download_from_kaggle\n",
        "\n",
        "path = download_from_kaggle()\n",
        "print(f\"Dataset em: {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset completo → vector store; sample_200 → test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de73c7fe34ea45e1903e903401f528a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vector store carregado de: /Users/moises/Documents/ticket-classifier/outputs/artifacts\n",
            "Classes: ['Access', 'Administrative rights', 'HR Support', 'Hardware', 'Internal Project', 'Miscellaneous', 'Purchase', 'Storage']\n",
            "Vector store (train): 47637 documentos\n",
            "Test set (sample_200): 200 tickets\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from src.prep import document_text, load_dataset, get_text_and_label_columns, stratified_sample\n",
        "from src.rag import VectorStore\n",
        "\n",
        "df_full = load_dataset()\n",
        "text_cols, label_col = get_text_and_label_columns(df_full)\n",
        "classes = sorted(set(df_full[label_col].astype(str)))\n",
        "\n",
        "n_sample = min(config.SAMPLE_SIZE, len(df_full))\n",
        "df_sample = stratified_sample(df_full, label_col, n=n_sample)\n",
        "df_sample.to_csv(config.DATA_PROCESSED / \"sample_200.csv\", index=False)\n",
        "ids_test = set(df_sample[\"id\"])\n",
        "df_train = df_full[~df_full[\"id\"].isin(ids_test)]\n",
        "\n",
        "texts_train = [document_text(row, text_cols) for _, row in df_train.iterrows()]\n",
        "labels_train = df_train[label_col].astype(str).tolist()\n",
        "ids_train = df_train[\"id\"].tolist()\n",
        "\n",
        "artifact_path = config.ARTIFACTS_DIR\n",
        "if (artifact_path / \"index.faiss\").exists():\n",
        "    store = VectorStore.load(artifact_path)\n",
        "    print(\"Vector store carregado de:\", artifact_path)\n",
        "else:\n",
        "    vc = VectorStore()\n",
        "    store = vc.build(texts_train, labels_train, ids=ids_train)\n",
        "    store.save(artifact_path)\n",
        "    print(\"Vector store construído (apenas train) e salvo em:\", artifact_path)\n",
        "\n",
        "texts = [document_text(row, text_cols) for _, row in df_sample.iterrows()]\n",
        "\n",
        "print(\"Classes:\", classes)\n",
        "print(\"Vector store (train): %d documentos\" % len(df_train))\n",
        "print(\"Test set (sample_200): %d tickets\" % len(df_sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distribuição por classe:\n",
            "Topic_group\n",
            "Hardware                 25\n",
            "Access                   25\n",
            "Miscellaneous            25\n",
            "HR Support               25\n",
            "Purchase                 25\n",
            "Administrative rights    25\n",
            "Storage                  25\n",
            "Internal Project         25\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Distribuição por classe:\")\n",
        "print(df_sample[label_col].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inferência em exemplos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb25f86e8cdf42bfa1c109078816a3f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Exemplos:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9c01104a6db47d3aa8d2f6fb03876b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Ticket 1 ---\n",
            "Texto (trecho): monitor request vulcan friday october pm hello please log each user monitor allocation user vulcan thank weekend engineer friday october vulcan parte  ...\n",
            "Saída: {'classe': 'Hardware', 'justificativa': \"Os vizinhos KNN sustentam a classe 'Hardware' porque todos os textos de ticket apresentam palavras relacionadas a equipamentos e sistemas, como 'monitor', 'vulcan', 'pc', 'engineer' e 'senior developer', o que sugere que a classe é relevante para a descrição do problema.\", 'classification_source': 'knn', 'confidence': 1.0, 'inference_time_sec': 20.60657862504013, 'classification_tokens': None, 'justification_tokens': 438}\n",
            "\n",
            "--- Ticket 2 ---\n",
            "Texto (trecho): stopped when docker start was executed sent wednesday february hi we having same problem we had few days ago server was stopped when executed docker s ...\n",
            "Saída: {'classe': 'Hardware', 'justificativa': \"Os vizinhos KNN sustentam a classe 'Hardware' porque os problemas relatados nos tickets são relacionados a falhas de hardware ou configurações de sistema, como problemas de acesso, servidor parado e instalação de software, o que sugere que a causa raiz do problema está relacionada a hardware ou configurações de sistema.\", 'classification_source': 'knn', 'confidence': 0.5714285714285714, 'inference_time_sec': 14.643519749981351, 'classification_tokens': None, 'justification_tokens': 378}\n",
            "\n",
            "--- Ticket 3 ---\n",
            "Texto (trecho): issue re access through for hello still work attached log error received during installation restarted machine disconnected tethered phone can connect ...\n",
            "Saída: {'classe': 'Hardware', 'justificativa': \"Os vizinhos KNN sustentam a classe 'Hardware' porque todos os relatórios de problemas apresentados pelos vizinhos contêm mensagens de erro relacionadas à instalação, conexão e autenticação, sugerindo que o problema está relacionado a problemas de hardware ou configuração do sistema.\", 'classification_source': 'knn', 'confidence': 1.0, 'inference_time_sec': 13.966664124978706, 'classification_tokens': None, 'justification_tokens': 434}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# store, classes e texts já carregados na célula 1.\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "from src.graph import build_pipeline, run_pipeline\n",
        "\n",
        "logging.getLogger(\"ticket_classifier.llm_usage\").setLevel(logging.WARNING)\n",
        "\n",
        "compiled, _, _ = build_pipeline(store, classes)\n",
        "\n",
        "for i in tqdm(range(min(3, len(texts))), desc=\"Exemplos\"):\n",
        "    out = run_pipeline(compiled, texts[i], classes)\n",
        "    print(f\"--- Ticket {i+1} ---\")\n",
        "    print(\"Texto (trecho):\", texts[i][:150], \"...\")\n",
        "    print(\"Saída:\", out)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Rodar na amostra de 200 e salvar resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'reset_usage_logger' from 'src.logging_utils' (/Users/moises/Documents/ticket-classifier/src/logging_utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m log_result, reset_usage_logger\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      4\u001b[39m results_path = config.OUTPUTS / \u001b[33m\"\u001b[39m\u001b[33mresults_sample.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'reset_usage_logger' from 'src.logging_utils' (/Users/moises/Documents/ticket-classifier/src/logging_utils/__init__.py)"
          ]
        }
      ],
      "source": [
        "from src.logging_utils import log_result\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "results_path = config.OUTPUTS / \"results_sample.jsonl\"\n",
        "if results_path.exists():\n",
        "    results_path.unlink()\n",
        "\n",
        "predictions = []\n",
        "for pos, (_, row) in enumerate(tqdm(list(df_sample.iterrows()), desc=\"Pipeline\")):\n",
        "    text = document_text(row, text_cols)\n",
        "    out = run_pipeline(compiled, text, classes, thread_id=str(pos), instance_id=row[\"id\"])\n",
        "    pred = out[\"classe\"]\n",
        "    predictions.append(pred)\n",
        "    log_result({\n",
        "        \"id\": row[\"id\"],\n",
        "        \"ticket_index\": pos,\n",
        "        \"true\": row[label_col],\n",
        "        \"pred\": pred,\n",
        "        \"justificativa\": out[\"justificativa\"],\n",
        "        \"classification_source\": out.get(\"classification_source\"),\n",
        "        \"confidence\": out.get(\"confidence\"),\n",
        "        \"inference_time_sec\": out.get(\"inference_time_sec\"),\n",
        "        \"classification_tokens\": out.get(\"classification_tokens\"),\n",
        "        \"justification_tokens\": out.get(\"justification_tokens\"),\n",
        "    })\n",
        "\n",
        "print(f\"Salvos {len(predictions)} resultados em {results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Métricas e relatório"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.metrics import compute_metrics, save_metrics_report\n",
        "\n",
        "y_true = df_sample[label_col].astype(str).tolist()\n",
        "metrics = compute_metrics(y_true, predictions, labels=classes)\n",
        "save_metrics_report(metrics)\n",
        "\n",
        "print(\"Accuracy:\", metrics[\"accuracy\"])\n",
        "print(\"F1 macro:\", metrics[\"f1_macro\"])\n",
        "print(\"F1 weighted:\", metrics[\"f1_weighted\"])\n",
        "print(\"\\nClassification report:\")\n",
        "for k, v in metrics[\"classification_report\"].items():\n",
        "    if isinstance(v, dict):\n",
        "        print(k, v)\n",
        "    else:\n",
        "        print(k, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Exemplo de saída JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "ex = run_pipeline(compiled, texts[0], classes)\n",
        "print(json.dumps(ex, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "ex = run_pipeline(compiled, texts[0], classes)\n",
        "print(json.dumps(ex, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ticket-classifier",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
