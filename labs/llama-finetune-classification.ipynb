{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning Llama 3.1 8B for IT Ticket Classification\n",
        "\n",
        "Fine-tuning do **Meta-Llama-3.1-8B-Instruct** para classificação de tickets. Dados: `dataset_with_id.csv` **excluindo** os IDs em `sample_200.csv` (mesmo protocolo de `labs/architecture-comparison.ipynb`). O modelo base é o mesmo do `./models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`; o treino usa o modelo completo via HuggingFace (LoRA); ao final pode-se exportar para GGUF.\n",
        "\n",
        "Contents:\n",
        "1. Setup e carregamento (excl. sample_200)\n",
        "2. Split estratificado e dataset de instrução\n",
        "3. LoRA fine-tuning (Transformers + PEFT)\n",
        "4. Salvamento do adapter e nota sobre export para GGUF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "ROOT = Path(\".\").resolve()\n",
        "if not (ROOT / \"data\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "dataset_path = ROOT / \"data\" / \"processed\" / \"dataset_with_id.csv\"\n",
        "sample_path = ROOT / \"data\" / \"processed\" / \"sample_200.csv\"\n",
        "\n",
        "df_full = pd.read_csv(dataset_path)\n",
        "sample_ids = set(pd.read_csv(sample_path)[\"id\"].astype(str))\n",
        "df = df_full[~df_full[\"id\"].astype(str).isin(sample_ids)].copy()\n",
        "df = df.reset_index(drop=True)\n",
        "tickets_df = df\n",
        "print(f\"Loaded {len(tickets_df)} rows (excluding {len(sample_ids)} sample_200 ids)\")\n",
        "tickets_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stratified split: 95% train+val, 5% test; then 80% train, 20% val (protocolo architecture-comparison)\n",
        "RANDOM_STATE = 28\n",
        "label_col = \"Topic_group\"\n",
        "tickets_df_train_val, tickets_df_test = train_test_split(\n",
        "    tickets_df, test_size=0.05, random_state=RANDOM_STATE, stratify=tickets_df[label_col]\n",
        ")\n",
        "tickets_df_train, tickets_df_val = train_test_split(\n",
        "    tickets_df_train_val, test_size=0.2, random_state=RANDOM_STATE, stratify=tickets_df_train_val[label_col]\n",
        ")\n",
        "print(\"Train:\", tickets_df_train.shape[0], \"Val:\", tickets_df_val.shape[0], \"Test:\", tickets_df_test.shape[0])\n",
        "classes = sorted(tickets_df[label_col].unique().tolist())\n",
        "print(\"Classes:\", classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Dataset de instrução para classificação\n",
        "\n",
        "Formato: instrução com o texto do ticket → resposta = categoria (exatamente um dos nomes de classe)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "CLASSIFICATION_INSTRUCTION = (\n",
        "    \"Classify the following IT support ticket into exactly one of these categories: {classes}. \"\n",
        "    \"Reply with only the category name.\\n\\nTicket:\\n{text}\"\n",
        ")\n",
        "\n",
        "def build_instruction(row, text_col=\"Document\", label_col=\"Topic_group\", classes_list=None):\n",
        "    classes_list = classes_list or classes\n",
        "    classes_str = \", \".join(classes_list)\n",
        "    text = (row[text_col] or \"\").strip()[:2000]\n",
        "    instruction = CLASSIFICATION_INSTRUCTION.format(classes=classes_str, text=text)\n",
        "    return instruction, row[label_col]\n",
        "\n",
        "def df_to_instruction_data(df, text_col=\"Document\", label_col=\"Topic_group\"):\n",
        "    instructions, labels = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        instr, label = build_instruction(row, text_col=text_col, label_col=label_col)\n",
        "        instructions.append(instr)\n",
        "        labels.append(label)\n",
        "    return pd.DataFrame({\"instruction\": instructions, \"response\": labels})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_inst = df_to_instruction_data(tickets_df_train)\n",
        "val_inst = df_to_instruction_data(tickets_df_val)\n",
        "print(\"Train samples:\", len(train_inst))\n",
        "print(train_inst.head(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. LoRA fine-tuning\n",
        "\n",
        "Requer: `pip install transformers peft datasets accelerate` (e, para 4-bit, `bitsandbytes`). Modelo base: **meta-llama/Meta-Llama-3.1-8B-Instruct** (mesma família do GGUF em `./models/`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Descomente para instalar dependências do fine-tuning:\n",
        "# !pip install transformers peft datasets accelerate\n",
        "# # Opcional (QLoRA): !pip install bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "OUTPUT_DIR = str(ROOT / \"outputs\" / \"artifacts\" / \"llama_finetune_classification\")\n",
        "ADAPTER_SAVE = str(ROOT / \"outputs\" / \"artifacts\" / \"llama_classification_adapter\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def format_chat(instruction: str, response: str) -> list[dict]:\n",
        "    return [\n",
        "        {\"role\": \"user\", \"content\": instruction},\n",
        "        {\"role\": \"assistant\", \"content\": response},\n",
        "    ]\n",
        "\n",
        "def tokenize_with_labels(examples, max_length=512):\n",
        "    texts = []\n",
        "    for instr, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        messages = format_chat(instr, resp)\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "    out = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    prompt_lengths = []\n",
        "    for instr, resp in zip(examples[\"instruction\"], examples[\"response\"]):\n",
        "        prompt_only = tokenizer.apply_chat_template(\n",
        "            format_chat(instr, \"\"), tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        enc = tokenizer(prompt_only, return_tensors=None)\n",
        "        prompt_lengths.append(len(enc[\"input_ids\"]))\n",
        "    labels = []\n",
        "    for i, input_ids in enumerate(out[\"input_ids\"]):\n",
        "        plen = min(prompt_lengths[i], len(input_ids))\n",
        "        labels.append([-100] * plen + input_ids[plen:])\n",
        "    out[\"labels\"] = labels\n",
        "    return out\n",
        "\n",
        "MAX_SEQ_LENGTH = 512\n",
        "train_ds = Dataset.from_pandas(train_inst).map(\n",
        "    lambda x: tokenize_with_labels(x, max_length=MAX_SEQ_LENGTH),\n",
        "    batched=True,\n",
        "    remove_columns=train_inst.columns.tolist(),\n",
        ")\n",
        "val_ds = Dataset.from_pandas(val_inst).map(\n",
        "    lambda x: tokenize_with_labels(x, max_length=MAX_SEQ_LENGTH),\n",
        "    batched=True,\n",
        "    remove_columns=val_inst.columns.tolist(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "def data_collator_fn(features):\n",
        "    batch = tokenizer.pad(\n",
        "        {\"input_ids\": [f[\"input_ids\"] for f in features], \"attention_mask\": [f[\"attention_mask\"] for f in features]},\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    batch[\"labels\"] = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
        "    batch[\"labels\"].masked_fill_(batch[\"labels\"] == tokenizer.pad_token_id, -100)\n",
        "    return batch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=data_collator_fn,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.makedirs(ADAPTER_SAVE, exist_ok=True)\n",
        "model.save_pretrained(ADAPTER_SAVE)\n",
        "tokenizer.save_pretrained(ADAPTER_SAVE)\n",
        "print(\"Adapter and tokenizer saved to:\", ADAPTER_SAVE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}